{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author of the code: Thiago Simoes Dias - tsimoesdias@gmail.com\n",
    "\n",
    "\n",
    "##### Public Dataset Information\n",
    "We express our gratitude to the authors for making their dataset publicly available:\n",
    "\n",
    "- **F. Wen, Z. Zhang, T. He, C. Lee** for their work *\"AI-enabled sign language recognition and VR space bidirectional communication using triboelectric smart glove\"* published in **Nature Communications**, which provided valuable insights and data for this analysis.  \n",
    "  DOI: [10.1038/s41467-021-25637-w](https://doi.org/10.1038/s41467-021-25637-w)\n",
    "\n",
    "- **F. Wen** for sharing the source data and dataset of *Fig. 2, Fig. 3, and Fig. 4* (2021).  \n",
    "  DOI: [10.7910/DVN/7KJWV3](https://doi.org/10.7910/DVN/7KJWV3)\n",
    "\n",
    "Their contributions significantly enhance research in the field, and we acknowledge their efforts with appreciation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "name= os.getcwd()\n",
    "\n",
    "df_0 = pd.read_excel(f\"{name}X0.xlsx\",header = None)\n",
    "df_1 = pd.read_excel(f\"{name}X1.xlsx\",header = None)\n",
    "df_2 = pd.read_excel(f\"{name}X2.xlsx\",header = None)\n",
    "df_3 = pd.read_excel(f\"{name}X3.xlsx\",header = None)\n",
    "df_4 = pd.read_excel(f\"{name}X4.xlsx\",header = None)\n",
    "df_5 = pd.read_excel(f\"{name}X5.xlsx\",header = None)\n",
    "df_6 = pd.read_excel(f\"{name}X6.xlsx\",header = None)\n",
    "df_7 = pd.read_excel(f\"{name}X7.xlsx\",header = None)\n",
    "df_8 = pd.read_excel(f\"{name}X8.xlsx\",header = None)\n",
    "df_9 = pd.read_excel(f\"{name}X9.xlsx\",header = None)\n",
    "\n",
    "df_10 = pd.read_excel(f\"{name}X10.xlsx\",header = None)\n",
    "df_11 = pd.read_excel(f\"{name}X11.xlsx\",header = None)\n",
    "df_12 = pd.read_excel(f\"{name}X12.xlsx\",header = None)\n",
    "df_13 = pd.read_excel(f\"{name}X13.xlsx\",header = None)\n",
    "df_14 = pd.read_excel(f\"{name}X14.xlsx\",header = None)\n",
    "df_15 = pd.read_excel(f\"{name}X15.xlsx\",header = None)\n",
    "df_16 = pd.read_excel(f\"{name}X16.xlsx\",header = None)\n",
    "df_17 = pd.read_excel(f\"{name}X17.xlsx\",header = None)\n",
    "df_18 = pd.read_excel(f\"{name}X18.xlsx\",header = None)\n",
    "df_19 = pd.read_excel(f\"{name}X19.xlsx\",header = None)\n",
    "\n",
    "df_20 = pd.read_excel(f\"{name}X20.xlsx\",header = None)\n",
    "df_21 = pd.read_excel(f\"{name}X21.xlsx\",header = None)\n",
    "df_22 = pd.read_excel(f\"{name}X22.xlsx\",header = None)\n",
    "df_23 = pd.read_excel(f\"{name}X23.xlsx\",header = None)\n",
    "df_24 = pd.read_excel(f\"{name}X24.xlsx\",header = None)\n",
    "df_25 = pd.read_excel(f\"{name}X25.xlsx\",header = None)\n",
    "df_26 = pd.read_excel(f\"{name}X26.xlsx\",header = None)\n",
    "df_27 = pd.read_excel(f\"{name}X27.xlsx\",header = None)\n",
    "df_28 = pd.read_excel(f\"{name}X28.xlsx\",header = None)\n",
    "df_29 = pd.read_excel(f\"{name}X29.xlsx\",header = None)\n",
    "\n",
    "df_30 = pd.read_excel(f\"{name}X30.xlsx\",header = None)\n",
    "df_31 = pd.read_excel(f\"{name}X31.xlsx\",header = None)\n",
    "df_32 = pd.read_excel(f\"{name}X32.xlsx\",header = None)\n",
    "df_33 = pd.read_excel(f\"{name}X33.xlsx\",header = None)\n",
    "df_34 = pd.read_excel(f\"{name}X34.xlsx\",header = None)\n",
    "df_35 = pd.read_excel(f\"{name}X35.xlsx\",header = None)\n",
    "df_36 = pd.read_excel(f\"{name}X36.xlsx\",header = None)\n",
    "df_37 = pd.read_excel(f\"{name}X37.xlsx\",header = None)\n",
    "df_38 = pd.read_excel(f\"{name}X38.xlsx\",header = None)\n",
    "df_39 = pd.read_excel(f\"{name}X39.xlsx\",header = None)\n",
    "\n",
    "df_40 = pd.read_excel(f\"{name}X40.xlsx\",header = None)\n",
    "df_41 = pd.read_excel(f\"{name}X41.xlsx\",header = None)\n",
    "df_42 = pd.read_excel(f\"{name}X42.xlsx\",header = None)\n",
    "df_43 = pd.read_excel(f\"{name}X43.xlsx\",header = None)\n",
    "df_44 = pd.read_excel(f\"{name}X44.xlsx\",header = None)\n",
    "df_45 = pd.read_excel(f\"{name}X45.xlsx\",header = None)\n",
    "df_46 = pd.read_excel(f\"{name}X46.xlsx\",header = None)\n",
    "df_47 = pd.read_excel(f\"{name}X47.xlsx\",header = None)\n",
    "df_48 = pd.read_excel(f\"{name}X48.xlsx\",header = None)\n",
    "df_49 = pd.read_excel(f\"{name}X49.xlsx\",header = None)\n",
    "\n",
    "\n",
    "gestos = [df_0,df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11,df_12,df_13,df_14,df_15,df_16,df_17,df_18,df_19,df_20,df_21,df_22,df_23,df_24,df_25,df_26,df_27,df_28,df_29,df_30,df_31,df_32,df_33,df_34,df_35,df_36,df_37,df_38,df_39,df_40,df_41,df_42,df_43,df_44,df_45,df_46,df_47,df_48,df_49]\n",
    "\n",
    "n_sensors = 15\n",
    "repeticoes = 100\n",
    "exemplos = 5000\n",
    "tam_jan = 200\n",
    "cont = 0\n",
    "df_all = np.random.rand(exemplos,tam_jan,n_sensors)*0\n",
    "\n",
    "for rep in range (repeticoes):\n",
    "    for ges in range(len(gestos)):\n",
    "        df_all[cont] = np.reshape(gestos[ges][rep][0:3000].to_numpy(),(200, -1))\n",
    "        cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handcrafted feature extraction across three different domains, using the TSFEL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfel\n",
    "\n",
    "#temporal columns\n",
    "name = \"_abs_energy\"\n",
    "abs_energy_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_distance\"\n",
    "distance_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_entropy\"\n",
    "entropy_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_mean_abs_diff\"\n",
    "mean_abs_diff_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_mean_diff\"\n",
    "mean_diff_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_median_abs_deviation\"\n",
    "median_abs_deviation_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_mean_abs_deviation\"\n",
    "mean_abs_deviation_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_median_abs_diff\"\n",
    "median_abs_diff_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_median_diff\"\n",
    "median_diff_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_negative_turning\"\n",
    "negative_turning_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_pk_pk_distance\"\n",
    "pk_pk_distance_col = [f\"{i}_{name}\" for i in range(1, 16)]\n",
    "name = \"_positive_turning\"\n",
    "positive_turning_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_slope\"\n",
    "slope_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_sum_abs_diff\"\n",
    "sum_abs_diff_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_neighbourhood_peaks\"\n",
    "neighbourhood_peaks_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "\n",
    "# Temporal\n",
    "dataset1 = pd.DataFrame(columns = [abs_energy_col])\n",
    "dataset2 = pd.DataFrame(columns = [distance_col])\n",
    "dataset3 = pd.DataFrame(columns = [entropy_col])\n",
    "dataset4 = pd.DataFrame(columns = [mean_abs_diff_col])\n",
    "dataset5 = pd.DataFrame(columns = [mean_diff_col])\n",
    "dataset6 = pd.DataFrame(columns = [median_abs_deviation_col])\n",
    "dataset7 = pd.DataFrame(columns = [mean_abs_deviation_col])\n",
    "dataset8 = pd.DataFrame(columns = [median_abs_diff_col])\n",
    "dataset9 = pd.DataFrame(columns = [median_diff_col])\n",
    "dataset10 = pd.DataFrame(columns = [negative_turning_col])\n",
    "dataset11 = pd.DataFrame(columns = [pk_pk_distance_col])\n",
    "dataset12 = pd.DataFrame(columns = [positive_turning_col])\n",
    "dataset13 = pd.DataFrame(columns = [slope_col])\n",
    "dataset14 = pd.DataFrame(columns = [sum_abs_diff_col])\n",
    "dataset15 = pd.DataFrame(columns = [neighbourhood_peaks_col])\n",
    "\n",
    "col_temporal = pd.concat([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8, dataset9, dataset10, dataset11, dataset12, dataset13, dataset14, dataset15])\n",
    "\n",
    "#statistical columns\n",
    "name = \"_calc_max\"\n",
    "calc_max_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_calc_mean\"\n",
    "calc_mean_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_calc_median\"\n",
    "calc_median_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_calc_min\"\n",
    "calc_min_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_calc_std\"\n",
    "calc_std_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_calc_var\"\n",
    "calc_var_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_interq_range\"\n",
    "interq_range_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_kurtosis\"\n",
    "kurtosis_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_total_energy\"\n",
    "total_energy_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_skewness\"\n",
    "skewness_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_rms\"\n",
    "rms_col = [f\"{i}_{name}\" for i in range(1, 16)]\n",
    "\n",
    "# STATISTICAL\n",
    "dataset1 = pd.DataFrame(columns = [calc_max_col])\n",
    "dataset2 = pd.DataFrame(columns = [calc_mean_col])\n",
    "dataset3 = pd.DataFrame(columns = [calc_median_col])\n",
    "dataset4 = pd.DataFrame(columns = [calc_min_col])\n",
    "dataset5 = pd.DataFrame(columns = [calc_std_col])\n",
    "dataset6 = pd.DataFrame(columns = [calc_var_col])\n",
    "dataset7 = pd.DataFrame(columns = [interq_range_col])\n",
    "dataset8 = pd.DataFrame(columns = [kurtosis_col])\n",
    "dataset9 = pd.DataFrame(columns = [total_energy_col])\n",
    "dataset10 = pd.DataFrame(columns = [skewness_col])\n",
    "dataset11 = pd.DataFrame(columns = [rms_col])\n",
    "\n",
    "col_statistical = pd.concat([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8, dataset9, dataset10, dataset11])\n",
    "\n",
    "#spectral columns\n",
    "name = \"_fundamental_frequency\"\n",
    "fundamental_frequency_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_human_range_energy\"\n",
    "human_range_energy_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_max_power_spectrum\"\n",
    "max_power_spectrum_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_max_frequency\"\n",
    "max_frequency_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_median_frequency\"\n",
    "median_frequency_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_power_bandwidth\"\n",
    "power_bandwidth_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_centroid\"\n",
    "spectral_centroid_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_decrease\"\n",
    "spectral_decrease_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_distance\"\n",
    "spectral_distance_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_entropy\"\n",
    "spectral_entropy_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_kurtosis\"\n",
    "spectral_kurtosis_col = [f\"{i}_{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_positive_turning\"\n",
    "spectral_positive_turning_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_roll_off\"\n",
    "spectral_roll_off_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_roll_on\"\n",
    "spectral_roll_on_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_skewness\"\n",
    "spectral_skewness_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_slope_col\"\n",
    "spectral_slope_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_spread\"\n",
    "spectral_spread_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "name = \"_spectral_variation\"\n",
    "spectral_variation_col = [f\"{i}{name}\" for i in range(1, 16)]\n",
    "\n",
    "# spectral\n",
    "dataset1 = pd.DataFrame(columns = [fundamental_frequency_col])\n",
    "dataset2 = pd.DataFrame(columns = [human_range_energy_col])\n",
    "dataset3 = pd.DataFrame(columns = [max_power_spectrum_col])\n",
    "dataset4 = pd.DataFrame(columns = [max_frequency_col])\n",
    "dataset5 = pd.DataFrame(columns = [median_frequency_col])\n",
    "dataset6 = pd.DataFrame(columns = [power_bandwidth_col])\n",
    "dataset7 = pd.DataFrame(columns = [spectral_centroid_col])\n",
    "dataset8 = pd.DataFrame(columns = [spectral_decrease_col])\n",
    "dataset9 = pd.DataFrame(columns = [spectral_distance_col])\n",
    "dataset10 = pd.DataFrame(columns = [spectral_entropy_col])\n",
    "dataset11 = pd.DataFrame(columns = [spectral_kurtosis_col])\n",
    "dataset12 = pd.DataFrame(columns = [spectral_positive_turning_col])\n",
    "dataset13 = pd.DataFrame(columns = [spectral_roll_off_col])\n",
    "dataset14 = pd.DataFrame(columns = [spectral_roll_on_col])\n",
    "dataset15 = pd.DataFrame(columns = [spectral_skewness_col])\n",
    "dataset16 = pd.DataFrame(columns = [spectral_slope_col])\n",
    "dataset17 = pd.DataFrame(columns = [spectral_spread_col])\n",
    "dataset18 = pd.DataFrame(columns = [spectral_variation_col])\n",
    "\n",
    "col_spectral = pd.concat([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8, dataset9, dataset10, dataset11, dataset12, dataset13, dataset14, dataset15, dataset16, dataset17, dataset18])\n",
    "\n",
    "def remove_offset(data):\n",
    "    mov_ = data.copy()\n",
    "    for i in range(15): #Remove a componente cc de cada sensor\n",
    "        mov_[i] = mov_[i] - mov_[i].mean()\n",
    "\n",
    "    return mov_\n",
    "\n",
    "def indices(data,tr):\n",
    "    threshold = tr # valor limiar\n",
    "    serie = data #carrega a janela de dados\n",
    "    idx_crossing = np.where(np.diff((serie > threshold).astype(int)) != 0)[0] + 1 \n",
    "\n",
    "    is_rising = serie[idx_crossing - 1] < threshold\n",
    "    starts = idx_crossing[is_rising]\n",
    "    ends = idx_crossing[~is_rising]\n",
    "\n",
    "    if len(starts) > 0:\n",
    "        min_ = min(starts) -15\n",
    "    else:\n",
    "        min_ = min([0,0])\n",
    "\n",
    "    if len(ends) > 0:\n",
    "        max_ = max(ends)+15\n",
    "    else:\n",
    "        max_ = max([200,200])\n",
    "\n",
    "    if (min_ < 0):\n",
    "        min_ = 0\n",
    "    \n",
    "    if (max_ > 199):\n",
    "        max_ = 200\n",
    "    \n",
    "    return min_,max_ \n",
    "\n",
    "from tsfel.feature_extraction import features as extr\n",
    "fs = 50 \n",
    "for i in range(5000):\n",
    "    mov = pd.DataFrame(df_all[i])\n",
    "    mov_ = remove_offset(mov)\n",
    "    env = pow(mov_, 2).T.sum().rolling(window=10).mean() \n",
    "\n",
    "    tr = 1\n",
    "    val_min,val_max = indices(env,tr)\n",
    "\n",
    "    # temporal feature list\n",
    "    abs_energy = []\n",
    "    distance = []\n",
    "    entropy = [] \n",
    "    mean_abs_diff = []\n",
    "    mean_diff = []\n",
    "    median_abs_deviation = []\n",
    "    mean_abs_deviation = []\n",
    "    median_abs_diff = []\n",
    "    median_diff = []\n",
    "    negative_turning = []\n",
    "    pk_pk_distance = []\n",
    "    positive_turning = []\n",
    "    slope = []\n",
    "    sum_abs_diff = []\n",
    "    neighbourhood_peaks = []       \n",
    "\n",
    "    # statistical features list\n",
    "    calc_max = [] \n",
    "    calc_mean = []\n",
    "    calc_median = []\n",
    "    calc_min = []\n",
    "    calc_std = []\n",
    "    calc_var = [] \n",
    "    interq_range = []\n",
    "    kurtosis = []\n",
    "    total_energy = []\n",
    "    skewness = []\n",
    "    rms = []\n",
    "\n",
    "    # spectral feature list\n",
    "    fundamental_frequency = []\n",
    "    human_range_energy = []\n",
    "    max_power_spectrum = [] \n",
    "    max_frequency = []\n",
    "    median_frequency = []\n",
    "    power_bandwidth = []\n",
    "    spectral_centroid = []\n",
    "    spectral_decrease = []\n",
    "    spectral_distance = []\n",
    "    spectral_entropy = []\n",
    "    spectral_kurtosis = []\n",
    "    spectral_positive_turning = []\n",
    "    spectral_roll_off = []\n",
    "    spectral_roll_on = []\n",
    "    spectral_skewness = []  \n",
    "    spectral_slope = [] \n",
    "    spectral_spread = [] \n",
    "    spectral_variation = [] \n",
    "\n",
    "    qde_canais = 15\n",
    "    for j in range(qde_canais):\n",
    "        signal = pd.DataFrame(df_all[i][val_min:val_max])[j]\n",
    "\n",
    "        #temporal domain 15 feat\n",
    "        abs_energy.append(extr.abs_energy(signal))\n",
    "        distance.append(extr.distance(signal)) \n",
    "        entropy.append(extr.entropy(signal))\n",
    "        mean_abs_diff.append(extr.mean_abs_diff(signal))\n",
    "        mean_diff.append(extr.mean_diff(signal))\n",
    "        median_abs_deviation.append(extr.median_abs_deviation(signal))\n",
    "        mean_abs_deviation.append(extr.mean_abs_deviation(signal))\n",
    "        median_abs_diff.append(extr.median_abs_diff(signal))\n",
    "        median_diff.append(extr.median_diff(signal))\n",
    "        negative_turning.append(extr.negative_turning(signal))\n",
    "        pk_pk_distance.append(extr.pk_pk_distance(signal))\n",
    "        positive_turning.append(extr.positive_turning(signal))\n",
    "        slope.append(extr.slope(signal))\n",
    "        sum_abs_diff.append(extr.sum_abs_diff(signal))\n",
    "        neighbourhood_peaks.append(extr.neighbourhood_peaks(signal))\n",
    "        temporal_feat = abs_energy+distance+entropy+mean_abs_diff+mean_diff+median_abs_deviation+mean_abs_deviation+median_abs_diff+median_diff+negative_turning+pk_pk_distance+positive_turning+slope+sum_abs_diff+neighbourhood_peaks\n",
    "        \n",
    "        #statistical domain 11 feat\n",
    "        calc_max.append(extr.calc_max(signal))\n",
    "        calc_mean.append(extr.calc_mean(signal))\n",
    "        calc_median.append(extr.calc_median(signal))\n",
    "        calc_min.append(extr.calc_min(signal))\n",
    "        calc_std.append(extr.calc_std(signal))\n",
    "        calc_var.append(extr.calc_var(signal))\n",
    "        interq_range.append(extr.interq_range(signal))\n",
    "        kurtosis.append(extr.kurtosis(signal))\n",
    "        total_energy.append(extr.total_energy(signal,fs))\n",
    "        skewness.append(extr.skewness(signal))\n",
    "        rms.append(extr.rms(signal))\n",
    "        statistical_feat = calc_max+calc_mean+calc_median+calc_min+calc_std+calc_var+interq_range+kurtosis+total_energy+skewness+rms  \n",
    "\n",
    "        #spectral domain 18 feat\n",
    "        fundamental_frequency.append(extr.fundamental_frequency(signal,fs))\n",
    "        human_range_energy.append(extr.human_range_energy(signal,fs))\n",
    "        max_power_spectrum.append(extr.max_power_spectrum(signal,fs))\n",
    "        max_frequency.append(extr.max_frequency(signal,fs))\n",
    "        median_frequency.append(extr.median_frequency(signal,fs))\n",
    "        power_bandwidth.append(extr.power_bandwidth(signal,fs))\n",
    "        spectral_centroid.append(extr.spectral_centroid(signal,fs))\n",
    "        spectral_decrease.append(extr.spectral_decrease(signal,fs))\n",
    "        spectral_distance.append(extr.spectral_distance(signal,fs))\n",
    "        spectral_entropy.append(extr.spectral_entropy(signal,fs))\n",
    "        spectral_kurtosis.append(extr.spectral_kurtosis(signal,fs))\n",
    "        spectral_positive_turning.append(extr.spectral_positive_turning(signal,fs))\n",
    "        spectral_roll_off.append(extr.spectral_roll_off(signal,fs))\n",
    "        spectral_roll_on.append(extr.spectral_roll_on(signal,fs))\n",
    "        spectral_skewness.append(extr.spectral_skewness(signal,fs))\n",
    "        spectral_slope.append(extr.spectral_slope(signal,fs))\n",
    "        spectral_spread.append(extr.spectral_spread(signal,fs))\n",
    "        spectral_variation.append(extr.spectral_variation(signal,fs))\n",
    "        spectral_feat = fundamental_frequency+human_range_energy+max_power_spectrum+max_frequency+median_frequency+power_bandwidth+spectral_centroid+spectral_decrease+spectral_distance+spectral_entropy+spectral_kurtosis+spectral_positive_turning+spectral_roll_off+spectral_roll_on+spectral_skewness+spectral_slope+spectral_spread+spectral_variation\n",
    "    \n",
    "    col_temporal.loc[len(col_temporal)] = temporal_feat \n",
    "    col_statistical.loc[len(col_statistical)] = statistical_feat \n",
    "    col_spectral.loc[len(col_spectral)] = spectral_feat \n",
    "\n",
    "#salva os dados\n",
    "col_temporal_statistical_spectral = pd.concat([col_temporal,col_statistical,col_spectral], axis=1)\n",
    "# salva dados feat\n",
    "col_temporal.to_csv('col_temporal_50Hz.csv', index=False) #features\n",
    "col_statistical.to_csv('col_statistical_50Hz.csv', index=False) #features\n",
    "col_spectral.to_csv('col_spectral_50Hz.csv', index=False) #features\n",
    "col_temporal_statistical_spectral.to_csv('col_temporal_statistical_spectral_50Hz.csv', index=False) #features\n",
    "\n",
    "#salva dados labels\n",
    "Y = np.random.rand(exemplos,1)*0\n",
    "flag_class = 1\n",
    "for i in range(exemplos):\n",
    "    Y[i] = flag_class\n",
    "    flag_class = flag_class + 1\n",
    "    if(flag_class == 51):\n",
    "        flag_class = 1\n",
    "\n",
    "Y_df = pd.DataFrame(Y)\n",
    "Y_df.to_csv('Y_labels_50Hz.csv', index=False) #labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering employed using Orange Data Mining software, with access through an 'FCBF' file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# FCBF\n",
    "temp_FCBF = pd.read_csv(\"seg_auto_50Hz_50feat/temporal_FCBF_50Hz_50feat.csv\")[2:] \n",
    "stat_FCBF = pd.read_csv(\"seg_auto_50Hz_50feat/statistical_FCBF_50Hz_50feat.csv\")[2:]\n",
    "spec_FCBF = pd.read_csv(\"seg_auto_50Hz_50feat/spectral_FCBF_50Hz_50feat.csv\")[2:]\n",
    "temp_stat_spec_FCBF = pd.read_csv(\"seg_auto_50Hz_50feat/temporal_statistical_spectral_FCBF_50Hz_50feat.csv\")[2:]\n",
    "\n",
    "distribuicao_ = pd.DataFrame(columns = ['modelo','features','fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5', 'fold_6', 'fold_7', 'fold_8', 'fold_9', 'fold_10', 'feat_names'])\n",
    "distribuicao_KNN = distribuicao_.copy()\n",
    "distribuicao_RF = distribuicao_.copy()\n",
    "distribuicao_SVM = distribuicao_.copy() \n",
    "distribuicao_MLP = distribuicao_.copy()\n",
    "\n",
    "dom = [\"temp\",\"stat\",\"spec\",\"temp_stat_spec\"]\n",
    "tec = [\"FCBF\"] \n",
    "ex = [1,2,3,4,5]\n",
    "\n",
    "for e in ex:\n",
    "    exec = e\n",
    "    print(\" ------------------ instance -----------------  \", e)\n",
    "    for t in tec:\n",
    "        tecnica = t\n",
    "        \n",
    "        for d in dom:\n",
    "            dominio = d\n",
    "            cont = 0\n",
    "            \n",
    "            # carrega os dados corretos de técnica de seleção e domínio\n",
    "            if t == \"relieff\" and d == \"temp\":\n",
    "                data_process = temp_RL.copy()\n",
    "            elif t == \"relieff\" and d == \"stat\":\n",
    "                data_process = stat_RL.copy()\n",
    "            elif t == \"relieff\" and d == \"spec\":\n",
    "                data_process = spec_RL.copy()\n",
    "            elif t == \"relieff\" and d == \"temp_stat_spec\":\n",
    "                data_process = temp_stat_spec_RL.copy()\n",
    "            \n",
    "            elif t == \"infogain\" and d == \"temp\":\n",
    "                data_process = temp_IG.copy()\n",
    "            elif t == \"infogain\" and d == \"stat\":\n",
    "                data_process = stat_IG.copy()\n",
    "            elif t == \"infogain\" and d == \"spec\":\n",
    "                data_process = spec_IG.copy()\n",
    "            elif t == \"infogain\" and d == \"temp_stat_spec\":\n",
    "                data_process = temp_stat_spec_IG.copy()\n",
    "\n",
    "            elif t == \"FCBF\" and d == \"temp\":\n",
    "                data_process = temp_FCBF.copy()\n",
    "            elif t == \"FCBF\" and d == \"stat\":\n",
    "                data_process = stat_FCBF.copy()\n",
    "            elif t == \"FCBF\" and d == \"spec\":\n",
    "                data_process = spec_FCBF.copy()\n",
    "            elif t == \"FCBF\" and d == \"temp_stat_spec\":\n",
    "                data_process = temp_stat_spec_FCBF.copy()\n",
    "\n",
    "            # print(\"dom:\",t)\n",
    "            for k in range(30,55,5):\n",
    "                cont += 1\n",
    "                caminho = f\"seg_auto_res_50Hz/\"\n",
    "\n",
    "                n_feat = k\n",
    "                l = list(data_process.columns[0:n_feat])\n",
    "                l.append(data_process.columns[-1])\n",
    "                \n",
    "                X = data_process[l]\n",
    "                X = shuffle(X,random_state=exec).reset_index().drop('index', axis=1) # 1-> 14\n",
    "                \n",
    "                y_shuffled = X['class']\n",
    "                X_train_shuffled = X.drop('class', axis=1)\n",
    "\n",
    "                ex, feat = X_train_shuffled.shape\n",
    "                n_features = feat\n",
    "\n",
    "                folds = 10\n",
    "                cv = StratifiedKFold(n_splits=folds, shuffle=False) \n",
    "                print(f\"Best {n_features} features selected with {tecnica} and {dominio} features:\")\n",
    "                columns_nm = X_train_shuffled.columns\n",
    "\n",
    "                model_name = \"KNN\"\n",
    "                model_KNN = KNeighborsClassifier(n_neighbors=1,weights='distance',metric='manhattan',n_jobs=-1) #GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "                n_scores = cross_val_score(model_KNN, X_train_shuffled, y_shuffled.values.astype(float).astype(int), scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "                distribuicao_KNN.loc[cont] = model_name,n_features,n_scores[0],n_scores[1],n_scores[2],n_scores[3],n_scores[4],n_scores[5],n_scores[6],n_scores[7],n_scores[8],n_scores[9],list(columns_nm)\n",
    "                y_pred = cross_val_predict(model_KNN, X_train_shuffled, y_shuffled.values.astype(float).astype(int), cv=cv, n_jobs=-1)\n",
    "                conf_mat_knn = confusion_matrix(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                acuracia_mc = accuracy_score(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                print(f'Accuracy - matriz conf:', acuracia_mc)\n",
    "                print(f'Accuracy - {model_name}:', n_scores, n_scores.mean())\n",
    "\n",
    "                model_name = \"RF\"\n",
    "                model_RF = RandomForestClassifier(n_estimators=30, random_state=1,n_jobs=-1) \n",
    "                n_scores = cross_val_score(model_RF, X_train_shuffled, y_shuffled.values.astype(float).astype(int), scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "                distribuicao_RF.loc[cont] = model_name,n_features,n_scores[0],n_scores[1],n_scores[2],n_scores[3],n_scores[4],n_scores[5],n_scores[6],n_scores[7],n_scores[8],n_scores[9],list(columns_nm)\n",
    "                y_pred = cross_val_predict(model_RF, X_train_shuffled, y_shuffled.values.astype(float).astype(int), cv=cv, n_jobs=-1)\n",
    "                conf_mat_rf = confusion_matrix(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                acuracia_mc = accuracy_score(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                print(f'Accuracy - matriz conf:', acuracia_mc)\n",
    "                print(f'Accuracy -  {model_name}:', n_scores, n_scores.mean())\n",
    "\n",
    "                model_name = \"SVM\"\n",
    "                model_SVM = SVC(kernel='linear',C=1.0,gamma='scale',max_iter=100,verbose=True,random_state=1) # Linear Kernel\n",
    "                n_scores = cross_val_score(model_SVM, X_train_shuffled, y_shuffled.values.astype(float).astype(int), scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "                distribuicao_SVM.loc[cont] = model_name,n_features,n_scores[0],n_scores[1],n_scores[2],n_scores[3],n_scores[4],n_scores[5],n_scores[6],n_scores[7],n_scores[8],n_scores[9],list(columns_nm)\n",
    "                y_pred = cross_val_predict(model_SVM, X_train_shuffled, y_shuffled.values.astype(float).astype(int), cv=cv, n_jobs=-1)\n",
    "                conf_mat_svm = confusion_matrix(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                acuracia_mc = accuracy_score(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                print(f'Accuracy - matriz conf:', acuracia_mc)\n",
    "                print(f'Accuracy - SVM:', n_scores, n_scores.mean())\n",
    "\n",
    "                model_name = \"MLP\"\n",
    "                # Fletcher-Gloss    (2*math.sqrt(n_entradas))+ n_saidas < neurons < (2*neur_input)+1\n",
    "                n_entradas = n_feat\n",
    "                n_saidas = 50\n",
    "                neurs = (2*n_entradas)+1\n",
    "                if neurs < ((2*math.sqrt(n_entradas))+ n_saidas):\n",
    "                    neurs = int((2*math.sqrt(n_entradas))+ n_saidas)\n",
    "                    \n",
    "                #learning_rate_init=0.001, early_stopping = True, validation_fraction = 0.2\n",
    "                model_MLP = MLPClassifier(solver = 'adam', hidden_layer_sizes = (neurs,), random_state=1, alpha=0.0001, max_iter=200,activation='relu') #Neural multilayer , early_stopping = True, validation_fraction = 0.2\n",
    "                n_scores = cross_val_score(model_MLP, X_train_shuffled, y_shuffled.values.astype(float).astype(int), scoring='accuracy', cv=cv)\n",
    "                distribuicao_MLP.loc[cont] = model_name,n_features,n_scores[0],n_scores[1],n_scores[2],n_scores[3],n_scores[4],n_scores[5],n_scores[6],n_scores[7],n_scores[8],n_scores[9],list(columns_nm)\n",
    "                y_pred = cross_val_predict(model_MLP, X_train_shuffled, y_shuffled.values.astype(float).astype(int), cv=cv, n_jobs=-1)\n",
    "                conf_mat_mlp = confusion_matrix(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                acuracia_mc = accuracy_score(y_shuffled.values.astype(float).astype(int), y_pred)\n",
    "                print(f'Accuracy - matriz conf:', acuracia_mc)\n",
    "                print(f'Accuracy - MLP:', n_scores, n_scores.mean())\n",
    "                print(f'---------------------------------------------')\n",
    "\n",
    "                #salva as distribuiçoes\n",
    "                pd.DataFrame(conf_mat_knn).to_csv(f\"{caminho}{tecnica}_{dominio}/ConfMat_knn_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                pd.DataFrame(conf_mat_rf).to_csv(f\"{caminho}{tecnica}_{dominio}/ConfMat_rf_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                pd.DataFrame(conf_mat_svm).to_csv(f\"{caminho}{tecnica}_{dominio}/ConfMat_svm_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                pd.DataFrame(conf_mat_mlp).to_csv(f\"{caminho}{tecnica}_{dominio}/ConfMat_mlp_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "\n",
    "                distribuicao_KNN.to_csv(f\"{caminho}{tecnica}_{dominio}/distribuicao_knn_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                distribuicao_RF.to_csv(f\"{caminho}{tecnica}_{dominio}/distribuicao_rf_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                distribuicao_SVM.to_csv(f\"{caminho}{tecnica}_{dominio}/distribuicao_svm_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")\n",
    "                distribuicao_MLP.to_csv(f\"{caminho}{tecnica}_{dominio}/distribuicao_mlp_{tecnica}_10fold_exec{exec}_nfeat{feat}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thiago_Notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
